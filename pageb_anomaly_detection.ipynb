{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "- [pageb_benchmarks.zip](https://ir.library.oregonstate.edu/concern/parent/47429f155/file_sets/1g05fh87w)\n",
    "\n",
    "\n",
    "# 要求\n",
    "使用[Python Outlier Detection (PyOD)](https://github.com/yzhao062/pyod)或其他已知的工具包来完成分析工作\n",
    "\n",
    "# 提交的内容\n",
    "- 完整的分析代码\n",
    "- 分析报告：展示分析的思路，详细过程，结果及你的分析\n",
    "- 所选择的数据集在README中说明，数据文件不要上传到Github中\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# timekeeping\n",
    "timekeeping = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "940\n"
     ]
    }
   ],
   "source": [
    "PAGEB_ROOT = 'pageb/benchmarks'\n",
    "benchmark_list = os.listdir(PAGEB_ROOT)\n",
    "print(len(benchmark_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据来源说明\n",
    "\n",
    "根据论文[1]可知，数据集中会引入4种不同的层次的不相关特征（i.e., noise）。\n",
    "\n",
    "要创建新的不相关特征，首先从原始母集中随机选择一个特征。 然后，对于原始数据集中的每个数据点，通过从原始数据点的值进行统一采样（替换）来为此特征选择一个值。 结果是新添加的特征与某些原始特征具有相同的边缘分布，但是其值不包含有关数据点异常状态的信息。这保留了真实数据的特质，同时允许引入噪声。\n",
    "\n",
    "为了简化确定需要多少不相关特征的过程，如果数据集已经具有$d$维特征，而我们想评估$d^{'}$维，即将成对平均距离增加一个因子$\\alpha$所需的维数，那么\n",
    "\\begin{equation}\n",
    "d^{'} = \\left( \\alpha \\sqrt{d}\\right)^2 ~ ~ ~ ~(1)\\,,\n",
    "\\end{equation}\n",
    "\n",
    "其中$\\alpha \\in \\{1.0, 1.2, 1.5, 2.0\\}$.\n",
    "\n",
    "[1] Emmott A, Das S, Dietterich T G, et al. A Meta-Analysis of the Anomaly Detection Problem[J]. arXiv: Artificial Intelligence, 2015."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随机选取一个csv文件，确定该数据集的原始特征有哪些？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30 entries, 0 to 29\n",
      "Data columns (total 16 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   point.id        30 non-null     object \n",
      " 1   motherset       30 non-null     object \n",
      " 2   origin          30 non-null     object \n",
      " 3   original.label  30 non-null     int64  \n",
      " 4   diff.score      30 non-null     float64\n",
      " 5   ground.truth    30 non-null     object \n",
      " 6   V               30 non-null     float64\n",
      " 7   V.1             30 non-null     float64\n",
      " 8   V.2             30 non-null     float64\n",
      " 9   V.3             30 non-null     float64\n",
      " 10  V.4             30 non-null     float64\n",
      " 11  V.5             30 non-null     float64\n",
      " 12  V.6             30 non-null     float64\n",
      " 13  V.7             30 non-null     float64\n",
      " 14  V.8             30 non-null     float64\n",
      " 15  V.9             30 non-null     float64\n",
      "dtypes: float64(11), int64(1), object(4)\n",
      "memory usage: 3.9+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>point.id</th>\n",
       "      <th>motherset</th>\n",
       "      <th>origin</th>\n",
       "      <th>original.label</th>\n",
       "      <th>diff.score</th>\n",
       "      <th>ground.truth</th>\n",
       "      <th>V</th>\n",
       "      <th>V.1</th>\n",
       "      <th>V.2</th>\n",
       "      <th>V.3</th>\n",
       "      <th>V.4</th>\n",
       "      <th>V.5</th>\n",
       "      <th>V.6</th>\n",
       "      <th>V.7</th>\n",
       "      <th>V.8</th>\n",
       "      <th>V.9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pageb_point_3544</td>\n",
       "      <td>pageb</td>\n",
       "      <td>multiclass</td>\n",
       "      <td>1</td>\n",
       "      <td>0.546693</td>\n",
       "      <td>nominal</td>\n",
       "      <td>-0.235923</td>\n",
       "      <td>-0.745876</td>\n",
       "      <td>-0.242177</td>\n",
       "      <td>-0.426234</td>\n",
       "      <td>0.272044</td>\n",
       "      <td>-1.183939</td>\n",
       "      <td>-0.017650</td>\n",
       "      <td>-0.280187</td>\n",
       "      <td>-0.386450</td>\n",
       "      <td>-0.625569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pageb_point_4573</td>\n",
       "      <td>pageb</td>\n",
       "      <td>multiclass</td>\n",
       "      <td>5</td>\n",
       "      <td>0.845264</td>\n",
       "      <td>anomaly</td>\n",
       "      <td>3.825138</td>\n",
       "      <td>2.060915</td>\n",
       "      <td>5.332560</td>\n",
       "      <td>-0.320025</td>\n",
       "      <td>-0.464919</td>\n",
       "      <td>-0.668298</td>\n",
       "      <td>0.019264</td>\n",
       "      <td>5.805619</td>\n",
       "      <td>9.252114</td>\n",
       "      <td>5.488889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pageb_point_1154</td>\n",
       "      <td>pageb</td>\n",
       "      <td>multiclass</td>\n",
       "      <td>1</td>\n",
       "      <td>0.151508</td>\n",
       "      <td>nominal</td>\n",
       "      <td>0.080523</td>\n",
       "      <td>0.300133</td>\n",
       "      <td>0.059718</td>\n",
       "      <td>-0.111419</td>\n",
       "      <td>-0.386159</td>\n",
       "      <td>0.491895</td>\n",
       "      <td>-0.061658</td>\n",
       "      <td>0.063817</td>\n",
       "      <td>0.293325</td>\n",
       "      <td>0.725230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pageb_point_4607</td>\n",
       "      <td>pageb</td>\n",
       "      <td>multiclass</td>\n",
       "      <td>5</td>\n",
       "      <td>0.844626</td>\n",
       "      <td>anomaly</td>\n",
       "      <td>3.825138</td>\n",
       "      <td>1.956314</td>\n",
       "      <td>5.127173</td>\n",
       "      <td>-0.324748</td>\n",
       "      <td>-0.487421</td>\n",
       "      <td>-0.972995</td>\n",
       "      <td>0.036925</td>\n",
       "      <td>5.500187</td>\n",
       "      <td>8.183820</td>\n",
       "      <td>4.371193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pageb_point_1248</td>\n",
       "      <td>pageb</td>\n",
       "      <td>multiclass</td>\n",
       "      <td>1</td>\n",
       "      <td>0.126937</td>\n",
       "      <td>nominal</td>\n",
       "      <td>-0.130441</td>\n",
       "      <td>0.831854</td>\n",
       "      <td>0.058068</td>\n",
       "      <td>0.305208</td>\n",
       "      <td>-0.583058</td>\n",
       "      <td>-1.019872</td>\n",
       "      <td>-0.061513</td>\n",
       "      <td>0.020522</td>\n",
       "      <td>0.087107</td>\n",
       "      <td>0.551898</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           point.id motherset      origin  original.label  diff.score  \\\n",
       "0  pageb_point_3544     pageb  multiclass               1    0.546693   \n",
       "1  pageb_point_4573     pageb  multiclass               5    0.845264   \n",
       "2  pageb_point_1154     pageb  multiclass               1    0.151508   \n",
       "3  pageb_point_4607     pageb  multiclass               5    0.844626   \n",
       "4  pageb_point_1248     pageb  multiclass               1    0.126937   \n",
       "\n",
       "  ground.truth         V       V.1       V.2       V.3       V.4       V.5  \\\n",
       "0      nominal -0.235923 -0.745876 -0.242177 -0.426234  0.272044 -1.183939   \n",
       "1      anomaly  3.825138  2.060915  5.332560 -0.320025 -0.464919 -0.668298   \n",
       "2      nominal  0.080523  0.300133  0.059718 -0.111419 -0.386159  0.491895   \n",
       "3      anomaly  3.825138  1.956314  5.127173 -0.324748 -0.487421 -0.972995   \n",
       "4      nominal -0.130441  0.831854  0.058068  0.305208 -0.583058 -1.019872   \n",
       "\n",
       "        V.6       V.7       V.8       V.9  \n",
       "0 -0.017650 -0.280187 -0.386450 -0.625569  \n",
       "1  0.019264  5.805619  9.252114  5.488889  \n",
       "2 -0.061658  0.063817  0.293325  0.725230  \n",
       "3  0.036925  5.500187  8.183820  4.371193  \n",
       "4 -0.061513  0.020522  0.087107  0.551898  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(PAGEB_ROOT, benchmark_list[0]))\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据以上的信息我们可以确定，pageb这个数据集的原始特征维度$d=10$(``V``, ``V.1``~``V.9``)。因此，由等式（1）可知，所有csv文件所包含的列数可能为$16=\\left(1.0 \\times \\sqrt{10}\\right)^2+6$, $20=\\left(1.2 \\times \\sqrt{10}\\right)^2+6$, $28=\\left(1.5 \\times \\sqrt{10}\\right)^2+6$, $46=\\left(2.0 \\times \\sqrt{10}\\right)^2+6$.\n",
    "\n",
    "下面我们遍历所有csv文件，验证一下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possible columns of all csv files: {16, 20, 28, 46}\n",
      "Total amount: 3180315\n"
     ]
    }
   ],
   "source": [
    "d_set = set()\n",
    "d_count = 0\n",
    "for i in range(len(benchmark_list)):\n",
    "    df = pd.read_csv(os.path.join(PAGEB_ROOT, benchmark_list[i]))\n",
    "    d_set.add(len(df.columns))\n",
    "    d_count += len(df)\n",
    "print('Possible columns of all csv files:', d_set)\n",
    "print('Total amount:', d_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据特征选择\n",
    "\n",
    "为了充分利用所提供的数据集完成离群点分析与异常检测，将提取所有csv文件共同的特征（即原始特征,``V``, ``V.1``~``V.9``）作为算法或模型的输入，用于检测该条数据是否属于异常点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORIGIN_FEATURES = ['V', 'V.1', 'V.2', 'V.3', 'V.4', 'V.5', 'V.6', 'V.7', 'V.8', 'V.9', 'ground.truth']\n",
    "def feature_section(benchmark_list):\n",
    "    concat_data = pd.DataFrame()\n",
    "    for i in benchmark_list:\n",
    "        df = pd.read_csv(os.path.join(PAGEB_ROOT, i))\n",
    "        concat_data = concat_data.append(df[ORIGIN_FEATURES])\n",
    "    return concat_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f24ef6409881>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconcat_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_section\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbenchmark_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbenchmark_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mconcat_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mconcat_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-60113f8c9ddf>\u001b[0m in \u001b[0;36mfeature_section\u001b[0;34m(benchmark_list)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbenchmark_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPAGEB_ROOT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mconcat_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconcat_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mORIGIN_FEATURES\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mconcat_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "concat_data = feature_section(benchmark_list=benchmark_list)\n",
    "concat_data.info()\n",
    "concat_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集划分\n",
    "train set : test set = 8 : 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(concat_data, test_size=0.2, random_state=2020)\n",
    "\n",
    "def data_label_split(data, label_column='ground.truth'):\n",
    "    x = data.drop(label_column, axis=1)\n",
    "    y = []\n",
    "    for i in data[label_column].values:\n",
    "        if i == 'nominal':\n",
    "            y.append(0)\n",
    "        else:\n",
    "            y.append(1)\n",
    "    y = np.array(y)\n",
    "    return x, y\n",
    "\n",
    "X_train, y_train = data_label_split(train)\n",
    "X_test, y_test = data_label_split(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.multiclass import type_of_target\n",
    "type_of_target(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE降维，用于可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "# T-SNE Implementation\n",
    "t0 = time.time()\n",
    "X_train_reduced_tsne = TSNE(n_components=2, random_state=2020).fit_transform(X_train.values)\n",
    "X_test_reduced_tsne = TSNE(n_components=2, random_state=2020).fit_transform(X_test.values)\n",
    "t1 = time.time()\n",
    "print(\"T-SNE took {:.2} s\".format(t1 - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型比较\n",
    "### 单一模型\n",
    "- KNN\n",
    "- PCA\n",
    "- LOF\n",
    "\n",
    "### 组合模型\n",
    "- **Average**: average scores of all detectors\n",
    "- **Maximization**: maximum score across all detectors.\n",
    "- **Average of Maximum (AOM)**\n",
    "- **Maximum of Average (MOA)**\n",
    "\n",
    "ref: https://github.com/yzhao062/pyod/tree/master/examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### kNN\n",
    "初始化一个 ``pyod.models.knn.KNN`` 检测器, 模型拟合, 然后给出预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the KNN detector\n",
    "from pyod.models.knn import KNN\n",
    "\n",
    "clf_name = 'KNN'\n",
    "clf = KNN()\n",
    "clf.fit(X_train)\n",
    "\n",
    "# get the prediction labels and outlier scores of the training data\n",
    "y_train_pred = clf.labels_  # binary labels (0: inliers, 1: outliers)\n",
    "y_train_scores = clf.decision_scores_  # raw outlier scores\n",
    "\n",
    "# get the prediction on the test data\n",
    "y_test_pred = clf.predict(X_test)  # outlier labels (0 or 1)\n",
    "y_test_scores = clf.decision_function(X_test)  # outlier scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用 ``ROC`` 和 ``Precision @ Rank`` 评估预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyod.utils.data import evaluate_print\n",
    "# evaluate and print the results\n",
    "print(\"\\nOn Training Data:\")\n",
    "evaluate_print(clf_name, y_train, y_train_scores)\n",
    "print(\"\\nOn Test Data:\")\n",
    "evaluate_print(clf_name, y_test, y_test_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可视化 ``KNN``的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyod.utils.example import visualize\n",
    "visualize(clf_name, X_train_reduced_tsne, y_train, X_test_reduced_tsne, y_test, y_train_pred,\n",
    "          y_test_pred, show_figure=True, save_figure=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA\n",
    "初始化一个 ``pyod.models.pca.PCA`` 检测器, 模型拟合, 然后给出预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train PCA detector\n",
    "from pyod.models.pca import PCA\n",
    "\n",
    "clf_name = 'PCA'\n",
    "clf = PCA(n_components=3)\n",
    "clf.fit(X_train)\n",
    "\n",
    "# get the prediction labels and outlier scores of the training data\n",
    "y_train_pred = clf.labels_  # binary labels (0: inliers, 1: outliers)\n",
    "y_train_scores = clf.decision_scores_  # raw outlier scores\n",
    "\n",
    "# get the prediction on the test data\n",
    "y_test_pred = clf.predict(X_test)  # outlier labels (0 or 1)\n",
    "y_test_scores = clf.decision_function(X_test)  # outlier scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用 ``ROC`` 和 ``Precision @ Rank`` 评估预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate and print the results\n",
    "print(\"\\nOn Training Data:\")\n",
    "evaluate_print(clf_name, y_train, y_train_scores)\n",
    "print(\"\\nOn Test Data:\")\n",
    "evaluate_print(clf_name, y_test, y_test_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可视化 ``PCA`` 的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(clf_name, X_train_reduced_tsne, y_train, X_test_reduced_tsne, y_test, y_train_pred,\n",
    "          y_test_pred, show_figure=True, save_figure=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LOF\n",
    "初始化一个 ``pyod.models.lof.LOF`` 检测器, 模型拟合, 然后给出预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train LOF detector\n",
    "from pyod.models.lof import LOF\n",
    "clf_name = 'LOF'\n",
    "clf = LOF()\n",
    "clf.fit(X_train)\n",
    "\n",
    "# get the prediction labels and outlier scores of the training data\n",
    "y_train_pred = clf.labels_  # binary labels (0: inliers, 1: outliers)\n",
    "y_train_scores = clf.decision_scores_  # raw outlier scores\n",
    "\n",
    "# get the prediction on the test data\n",
    "y_test_pred = clf.predict(X_test)  # outlier labels (0 or 1)\n",
    "y_test_scores = clf.decision_function(X_test)  # outlier scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用 ``ROC`` 和 ``Precision @ Rank`` 评估预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate and print the results\n",
    "print(\"\\nOn Training Data:\")\n",
    "evaluate_print(clf_name, y_train, y_train_scores)\n",
    "print(\"\\nOn Test Data:\")\n",
    "evaluate_print(clf_name, y_test, y_test_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可视化 ``LOF`` 的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the results\n",
    "visualize(clf_name, X_train_reduced_tsne, y_train, X_test_reduced_tsne, y_test, y_train_pred,\n",
    "          y_test_pred, show_figure=True, save_figure=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Combination\n",
    "用不同的k(10 ～ 200)初始化20个 ``kNN`` 离群点检测器，然后得到所有的离群点的分数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyod.models.knn import KNN  # kNN detector\n",
    "from pyod.models.combination import aom, moa, average, maximization\n",
    "from pyod.utils.utility import standardizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardizing data for processing\n",
    "X_train_norm, X_test_norm = standardizer(X_train, X_test)\n",
    "\n",
    "n_clf = 20  # number of base detectors\n",
    "\n",
    "# initialize 20 base detectors for combination\n",
    "k_list = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200]\n",
    "\n",
    "train_scores = np.zeros([X_train.shape[0], n_clf])\n",
    "test_scores = np.zeros([X_test.shape[0], n_clf])\n",
    "\n",
    "print('Combining {n_clf} kNN detectors'.format(n_clf=n_clf))\n",
    "\n",
    "for i in range(n_clf):\n",
    "    k = k_list[i]\n",
    "\n",
    "    clf = KNN(n_neighbors=k, method='largest')\n",
    "    clf.fit(X_train_norm)\n",
    "\n",
    "    train_scores[:, i] = clf.decision_scores_\n",
    "    test_scores[:, i] = clf.decision_function(X_test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision scores have to be normalized before combination\n",
    "train_scores_norm, test_scores_norm = standardizer(train_scores, test_scores)\n",
    "\n",
    "# Combination by average\n",
    "y_by_average = average(test_scores_norm)\n",
    "# Combination by max\n",
    "y_by_maximization = maximization(test_scores_norm)\n",
    "# Combination by aom\n",
    "y_by_aom = aom(test_scores_norm, n_buckets=5)\n",
    "# Combination by moa\n",
    "y_by_moa = moa(test_scores_norm, n_buckets=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nOn Test Data:\")\n",
    "evaluate_print('Combination by Average', y_test, y_by_average)\n",
    "evaluate_print('Combination by Maximization', y_test, y_by_maximization)\n",
    "evaluate_print('Combination by AOM', y_test, y_by_aom)\n",
    "evaluate_print('Combination by MOA', y_test, y_by_moa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, s = divmod(time.time()-timekeeping, 60)\n",
    "h, m = divmod(m, 60)\n",
    "print ('run time: %02d:%02d:%02d' % (h, m, s))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
